import streamlit as st
import re
import openai
# import config
from dotenv import load_dotenv
import os
from services.llm_service import process_certificate_field, process_award_field, process_education_field, process_experience_field

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_TYPE = os.getenv("OPENAI_API_TYPE")
OPENAI_API_VERSION = os.getenv("OPENAI_API_VERSION")
AZURE_ENDPOINT = os.getenv("AZURE_ENDPOINT")

AZURE_SEARCH_SERVICE_NAME = os.getenv("AZURE_SEARCH_SERVICE_NAME")
AZURE_SEARCH_INDEX_NAME = os.getenv("AZURE_SEARCH_INDEX_NAME")
AZURE_SEARCH_API_KEY = os.getenv("AZURE_SEARCH_API_KEY")
AZURE_SEARCH_API_VERSION = os.getenv("AZURE_SEARCH_API_VERSION")

# LangChain ê´€ë ¨ import (ì„ íƒì )
try:
    from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
    from langchain_community.retrievers import AzureCognitiveSearchRetriever
    from langchain.chains import RetrievalQA
    from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False

def analyze_resume_for_question(question, analysis_results):
    """
    ì´ë ¥ì„œ ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extract_keywords_from_question(question)
        
        # ì§€ì›ìë“¤ì˜ ì •ë³´ë¥¼ êµ¬ì¡°í™”
        candidates_info = []
        for result in analysis_results:
            file_name = result["file_name"]
            analysis = result["analysis"]
            
            candidate_info = {
                "file_name": file_name,
                "fields": {},
                "fitness_score": result.get("fitness_score")
            }
            
            # í•„ë“œ ì •ë³´ ì¶”ì¶œ
            if analysis["documents"] and analysis["documents"][0]["fields"]:
                fields = analysis["documents"][0]["fields"]
                
                # ì£¼ìš” í•„ë“œë“¤ ì²˜ë¦¬
                for field_name in ["í•™ë ¥ì‚¬í•­", "ê²½ë ¥ì‚¬í•­", "ìê²©ì¦", "ìˆ˜ìƒê²½ë ¥", "ê¸°ë³¸ì •ë³´"]:
                    if field_name in fields:
                        field_content = fields[field_name]['content']
                        
                        # êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜
                        if field_name == "í•™ë ¥ì‚¬í•­":
                            candidate_info["fields"][field_name] = process_education_field(field_content)
                        elif field_name == "ê²½ë ¥ì‚¬í•­":
                            candidate_info["fields"][field_name] = process_experience_field(field_content)
                        elif field_name == "ìê²©ì¦":
                            candidate_info["fields"][field_name] = process_certificate_field(field_content)
                        elif field_name == "ìˆ˜ìƒê²½ë ¥":
                            candidate_info["fields"][field_name] = process_award_field(field_content)
                        else:
                            candidate_info["fields"][field_name] = field_content
            
            candidates_info.append(candidate_info)
        
        # ì§ˆë¬¸ì— ë§ëŠ” ì§€ì›ì ë¶„ì„
        if keywords:
            return analyze_candidates_by_keywords(question, keywords, candidates_info)
        
        return None
        
    except Exception as e:
        st.error(f"ì´ë ¥ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def extract_keywords_from_question(question):
    """
    ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    keywords = []
    
    # ê¸°ìˆ  ìŠ¤íƒ ê´€ë ¨ í‚¤ì›Œë“œ
    tech_keywords = [
        "java", "python", "javascript", "react", "vue", "angular", "node.js", "spring",
        "django", "flask", "mysql", "postgresql", "mongodb", "redis", "docker", "kubernetes",
        "aws", "azure", "gcp", "git", "jenkins", "jira", "agile", "scrum"
    ]
    
    # ê²½í—˜ ê´€ë ¨ í‚¤ì›Œë“œ
    experience_keywords = [
        "ê²½ë ¥", "ê²½í—˜", "í”„ë¡œì íŠ¸", "ê°œë°œ", "í”„ë¡œê·¸ë˜ë°", "ì½”ë”©", "ì‹œë‹ˆì–´", "ì£¼ë‹ˆì–´",
        "ì‹ ì…", "ì¤‘ê¸‰", "ê³ ê¸‰", "ë¦¬ë“œ", "ë§¤ë‹ˆì €", "íŒ€ì¥"
    ]
    
    # í•™ë ¥ ê´€ë ¨ í‚¤ì›Œë“œ
    education_keywords = [
        "í•™ë ¥", "í•™ìœ„", "ëŒ€í•™êµ", "ëŒ€í•™ì›", "ì„ì‚¬", "ë°•ì‚¬", "í•™ì‚¬", "ì „ê³µ"
    ]
    
    # ìê²©ì¦ ê´€ë ¨ í‚¤ì›Œë“œ
    certificate_keywords = [
        "ìê²©ì¦", "ì¸ì¦", "certificate", "license", "aws", "azure", "oracle", "microsoft"
    ]
    
    question_lower = question.lower()
    
    # ê¸°ìˆ  ìŠ¤íƒ í‚¤ì›Œë“œ ê²€ìƒ‰
    for keyword in tech_keywords:
        if keyword in question_lower:
            keywords.append(keyword)
    
    # ê²½í—˜ í‚¤ì›Œë“œ ê²€ìƒ‰
    for keyword in experience_keywords:
        if keyword in question_lower:
            keywords.append(keyword)
    
    # í•™ë ¥ í‚¤ì›Œë“œ ê²€ìƒ‰
    for keyword in education_keywords:
        if keyword in question_lower:
            keywords.append(keyword)
    
    # ìê²©ì¦ í‚¤ì›Œë“œ ê²€ìƒ‰
    for keyword in certificate_keywords:
        if keyword in question_lower:
            keywords.append(keyword)
    
    return keywords

def analyze_candidates_by_keywords(question, keywords, candidates_info):
    """
    í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§€ì›ìë“¤ì„ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        matching_candidates = []
        
        for candidate in candidates_info:
            score = 0
            matched_keywords = []
            
            # ê° í‚¤ì›Œë“œì— ëŒ€í•´ ì ìˆ˜ ê³„ì‚°
            for keyword in keywords:
                candidate_text = ""
                
                # ëª¨ë“  í•„ë“œì˜ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
                for field_name, field_content in candidate["fields"].items():
                    if isinstance(field_content, str):
                        candidate_text += field_content + " "
                    elif isinstance(field_content, list):
                        for item in field_content:
                            if isinstance(item, dict):
                                candidate_text += str(item) + " "
                            else:
                                candidate_text += str(item) + " "
                    else:
                        candidate_text += str(field_content) + " "
                
                candidate_text_lower = candidate_text.lower()
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
                if keyword in candidate_text_lower:
                    score += 1
                    matched_keywords.append(keyword)
            
            # ë§¤ì¹­ëœ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê²°ê³¼ì— ì¶”ê°€
            if score > 0:
                candidate["match_score"] = score
                candidate["matched_keywords"] = matched_keywords
                matching_candidates.append(candidate)
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        matching_candidates.sort(key=lambda x: x["match_score"], reverse=True)
        
        return matching_candidates
        
    except Exception as e:
        st.error(f"ì§€ì›ì ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

@st.cache_resource
def get_llm():
    """Azure OpenAI LLMì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        llm = AzureChatOpenAI(
            openai_api_version=OPENAI_API_VERSION,
            azure_deployment="gpt-4.1",
            azure_endpoint=AZURE_ENDPOINT,
            api_key=OPENAI_API_KEY,
            temperature=0.1
        )
        return llm
    except Exception as e:
        st.error(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return None

@st.cache_resource(ttl=60)
def get_embedding_model():
    """Azure OpenAI Embedding ëª¨ë¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        embeddings = AzureOpenAIEmbeddings(
            azure_deployment="text-embedding-3-large",
            azure_endpoint=AZURE_ENDPOINT,
            api_key=OPENAI_API_KEY,
            openai_api_version=OPENAI_API_VERSION
        )
        return embeddings
    except Exception as e:
        st.error(f"Embedding ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return None

@st.cache_resource(ttl=60)
def get_retriever():
    """Azure AI Search ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # .env íŒŒì¼ì—ì„œ ì§ì ‘ í™˜ê²½ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        azure_search_service_name = AZURE_SEARCH_SERVICE_NAME
        azure_search_index_name = AZURE_SEARCH_INDEX_NAME
        azure_search_api_key = AZURE_SEARCH_API_KEY
        azure_search_api_version = AZURE_SEARCH_API_VERSION
        
        retriever = AzureCognitiveSearchRetriever(
            service_name=AZURE_SEARCH_SERVICE_NAME,
            index_name=AZURE_SEARCH_INDEX_NAME,
            api_key=AZURE_SEARCH_API_KEY,
            api_version=AZURE_SEARCH_API_VERSION,
            top_k=10,
            content_key="chunk"
        )
        return retriever
    except Exception as e:
        st.error(f"ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return None

@st.cache_resource
def get_qa_chain():
    """RetrievalQA ì²´ì¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        llm = get_llm()
        retriever = get_retriever()
        
        if not llm or not retriever:
            return None
            
        # Few-shot ì˜ˆì‹œ ë°ì´í„° ì •ì˜
        examples = [
            {
                "question": "ë°±ì—”ë“œ ê°œë°œ ê²½í—˜ì´ ìˆëŠ” ì§€ì›ìë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "context": "ì§€ì›ì A: Java/Spring ë°±ì—”ë“œ ê°œë°œ 3ë…„ ê²½í—˜\nì§€ì›ì B: Node.js/Express ë°±ì—”ë“œ ê°œë°œ 2ë…„ ê²½í—˜",
                "answer": "ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í™•ì¸í•œ ê²°ê³¼, ë°±ì—”ë“œ ê°œë°œ ê²½í—˜ì´ ìˆëŠ” ì§€ì›ìë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n- ì§€ì›ì A: Java/Springì„ ì‚¬ìš©í•œ ë°±ì—”ë“œ ê°œë°œ 3ë…„ ê²½í—˜\n- ì§€ì›ì B: Node.js/Express ë°±ì—”ë“œ ê°œë°œ 2ë…„ ê²½í—˜"
            },
            {
                "question": "ë¨¸ì‹ ëŸ¬ë‹ ì „ë¬¸ê°€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "context": "ì§€ì›ì A: ì›¹ ê°œë°œ ê²½í—˜\nì§€ì›ì B: ëª¨ë°”ì¼ ì•± ê°œë°œ ê²½í—˜",
                "answer": "ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í™•ì¸í–ˆì§€ë§Œ, ë¨¸ì‹ ëŸ¬ë‹ ê´€ë ¨ ê²½í—˜ì´ ìˆëŠ” ì§€ì›ì ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            },
            {
                "question": "ì•ˆë…•í•˜ì„¸ìš”",
                "context": "",
                "answer": "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì±„ìš© ì§€ì› ì‹œìŠ¤í…œì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n\nì§€ì›ìì™€ ì±„ìš©ê³µê³ ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´:\n\nâ€¢ ì§€ì›ìë“¤ì˜ í•™ë ¥ì‚¬í•­ì´ë‚˜ ê²½ë ¥ì‚¬í•­ì„ ë¬¼ì–´ë³´ì‹¤ ìˆ˜ ìˆì–´ìš”\nâ€¢ íŠ¹ì • ì§€ì›ìì˜ ì í•©ì„±ì´ë‚˜ í‰ê°€ ê¸°ì¤€ì„ ì•Œì•„ë³´ì‹¤ ìˆ˜ ìˆì–´ìš”\nâ€¢ ë©´ì ‘ì—ì„œ ë¬¼ì–´ë³¼ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ë“¤ì„ ì•Œì•„ë³´ì‹¤ ìˆ˜ ìˆì–´ìš”\nâ€¢ íŠ¹ì • ê¸°ìˆ ì´ë‚˜ ê²½í—˜ì„ ê°€ì§„ ì§€ì›ìë¥¼ ì°¾ì•„ë³´ì‹¤ ìˆ˜ ìˆì–´ìš”\n\nê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  í¸í•˜ê²Œ ë¬¼ì–´ë³´ì„¸ìš”!"
            }
        ]

        # ì˜ˆì‹œ í¬ë§·íŒ… í…œí”Œë¦¿
        example_prompt = ChatPromptTemplate.from_messages([
            ("human", "ì§ˆë¬¸: {question}\n\nì§€ì›ì ì •ë³´:\n{context}"),
            ("assistant", "{answer}")
        ])

        # Few-shot í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
        few_shot_prompt = FewShotChatMessagePromptTemplate(
            example_prompt=example_prompt,
            examples=examples
        )

        # ìµœì¢… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±
        final_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì±„ìš© ì§€ì› ì‹œìŠ¤í…œì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì§€ì›ìì™€ ì±„ìš©ê³µê³ ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. ì¼ë°˜ì ì¸ ì¸ì‚¬ë§ì´ë‚˜ ëŒ€í™”ì—ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
   "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì±„ìš© ì§€ì› ì‹œìŠ¤í…œì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n\nì§€ì›ìì™€ ì±„ìš©ê³µê³ ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´:\n\nâ€¢ ì§€ì›ìë“¤ì˜ í•™ë ¥ì‚¬í•­ì´ë‚˜ ê²½ë ¥ì‚¬í•­ì„ ë¬¼ì–´ë³´ì‹¤ ìˆ˜ ìˆì–´ìš”\nâ€¢ íŠ¹ì • ì§€ì›ìì˜ ì í•©ì„±ì´ë‚˜ í‰ê°€ ê¸°ì¤€ì„ ì•Œì•„ë³´ì‹¤ ìˆ˜ ìˆì–´ìš”\nâ€¢ ë©´ì ‘ì—ì„œ ë¬¼ì–´ë³¼ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ë“¤ì„ ì•Œì•„ë³´ì‹¤ ìˆ˜ ìˆì–´ìš”\nâ€¢ íŠ¹ì • ê¸°ìˆ ì´ë‚˜ ê²½í—˜ì„ ê°€ì§„ ì§€ì›ìë¥¼ ì°¾ì•„ë³´ì‹¤ ìˆ˜ ìˆì–´ìš”\n\nê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  í¸í•˜ê²Œ ë¬¼ì–´ë³´ì„¸ìš”!"
2. ì§€ì›ìë‚˜ ì±„ìš©ê³µê³ ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë©´, ì œê³µëœ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
3. íŠ¹ì • ê¸°ìˆ ì´ë‚˜ ê²½í—˜ì— ëŒ€í•œ ì§ˆë¬¸ì¸ ê²½ìš°, í•´ë‹¹ ê¸°ìˆ ì„ ì‚¬ìš©í•œ ê²½í—˜ì´ë‚˜ ê´€ë ¨ ìê²©ì¦ì„ ê°€ì§„ ì§€ì›ìë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì¶”ì²œí•´ì£¼ì„¸ìš”.
4. ê²½ë ¥ì‚¬í•­ì—ì„œ í•´ë‹¹ ê¸°ìˆ ì„ ì‚¬ìš©í•œ êµ¬ì²´ì ì¸ í”„ë¡œì íŠ¸ë‚˜ ì—…ë¬´ ë‚´ìš©ì„ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
5. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”.
6. ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ì„¤ëª…ì„ í¬í•¨í•´ì£¼ì„¸ìš”.
7. ë¶„ì„ë˜ì§€ ì•Šì€ ì´ë ¥ì„œë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì§€ì›ìì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.
8. ì œê³µëœ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
9. ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš° "í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
10. ê²½ë ¥ì‚¬í•­, ìê²©ì¦, í•™ë ¥ì‚¬í•­, ìˆ˜ìƒê²½ë ¥ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.
11. ê°€ì¥ ì í•©í•œ ì§€ì›ìë¶€í„° ìˆœì„œëŒ€ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""),
            few_shot_prompt,
            ("human", "ë‹¤ìŒì€ ì§€ì›ìì˜ ì´ë ¥ ì •ë³´ ë° ì±„ìš©ê³µê³ ì™€ ê´€ë ¨ëœ ë¬¸ì„œë“¤ì…ë‹ˆë‹¤:\n\n<ì§€ì›ì ì •ë³´ ë° ê²€ìƒ‰ëœ ë¬¸ì„œ>\n{context}\n\n---\n\nì‚¬ìš©ì ì§ˆë¬¸: {question}")
        ])

        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=retriever,
            chain_type="stuff",
            chain_type_kwargs={"prompt": final_prompt},
            return_source_documents=True
        )
        return qa_chain
    except Exception as e:
        st.error(f"QA ì²´ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return None

def chat_with_llm():
    """ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤."""
    st.subheader("ğŸ¤– AI ì±—ë´‡")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # ì´ì „ ë©”ì‹œì§€ë“¤ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            try:
                # QA ì²´ì¸ ê°€ì ¸ì˜¤ê¸°
                qa_chain = get_qa_chain()
                
                if qa_chain:
                    # ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
                    response = qa_chain.invoke({"query": prompt})
                    
                    if response and "result" in response:
                        full_response = response["result"]
                    else:
                        full_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                else:
                    full_response = "ì£„ì†¡í•©ë‹ˆë‹¤. AI ì„œë¹„ìŠ¤ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
            except Exception as e:
                full_response = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            
            # ì‘ë‹µ í‘œì‹œ
            message_placeholder.markdown(full_response)
        
        # AI ì‘ë‹µì„ ì„¸ì…˜ì— ì¶”ê°€
        st.session_state.messages.append({"role": "assistant", "content": full_response})
    
    # ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun() 